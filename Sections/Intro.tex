\section{Introduction}

%OLD VERSION
%Deep neural nets have progressed at an amazing pace over the past decade. The community and the technology has been breaking grounds across the fields. When it comes to classification and inference for specific tasks like object detection, scene detection, language modeling etc, Deep learning has been at the fore front. But the  reasons behind a particular decision taken by a deep model, more or less still remains a mystery. This gap is by some accounts call the explainability gap, or the black box problem of Neural nets. Explainability and understanding the deep reasoning behind decisions is one of the most researched problems in the machine learning community.
%\par
%The problem of explain-ability becomes even more abstract and obscured, when we are dealing with tasks that handle meta, and abstract quantities like sentiment, affects and aesthetics. Despite the black box like nature, deep neural networks have done remarkable strides in understanding creativity~\cite{redi20146}, memorability~\cite{Isola2011} or beauty~\cite{schifanella2015image} at a meta level, and works great in providing inferences. These works explore perceptual qualities of media objects using deep learning, and treat the explainability of the models using round about methods like perturbation of input and understanding correlation of several governing variables with decisions of the network etc. These methods are perfectly valid and do give some interesting insights into the decision influencing factors for the models, however still fail to explain the knowledge that the network has learn't and which it uses to drive the decisions that it takes. 
%\par 
%Despite these issues, computational aesthetics have reaped a lot of benefits from the advances in Deep neural networks. Works like~\cite{khosla2014makes,quercia2014aesthetic,naik2014streetscore,parislooklikeparis}, underscore the utility of machine learning in computational aesthetics in urban settings. In this paper we propose a generalizable pipeline for analysing geo-referenced images (FaceLift) for affective and/or aesthetic metrics. For the purpose of specificity, we conduct all our studies on the dimension of beauty in urban images. The pipeline learns the concept urban affect and then for a given street-view image finds an approximate transformation so as to maximize or minimize the presence of said affect. The is done by generating synthetic images to maximize the affect and then finding a best match to the synthetic images from the street-view database under some compositional constraints. We show that the humans overwhelmingly agree with the pipeline's transformations using a crowd sourcing experiment and then show that the transformations of the pipeline can be explained using well known urban design metrics.

%Version 2 ===============
%The development of new technologies behind deep neural networks has progressed at an amazing pace over the past decade~\cite{lecun2015deep}, thus enabling the research community to achieve groundbreaking results across fields and in a wide variety of tasks including object detection~\cite{szegedy2013deep}, scene detection, reinforcement learning~\cite{mnih2015human}, and language modeling~\cite{arisoy2012deep}. Where deep learning excels in performance -- in many cases surpassing human ability -- it often falls short in producing models that are interpretable. As a result, unlike in traditional machine learning, neural networks are often used as black-box functions whose decisions cannot be supplemented by any human-readable explanation about \textit{why} those decisions were taken. This limitation is known as the \textit{explainability gap} or the \textit{black-box problem} of neural networks. The intrinsic complexity behind deep neural networks makes this problem hard to solve. However, as the adoption deep neural networks spreads across domains and as their use becomes more common to solve increasingly sophisticated duties with humans in the loop, it becomes crucial to annotate any decision with a human-readable explanation.
%
%In the domain of image processing, the problem of explainability becomes particularly hard to tackle when dealing with the measurement of intangible aspects like sentiment, affects, and aesthetics. Despite its opaque nature, deep neural networks have done remarkable strides in capturing creativity~\cite{redi20146}, memorability~\cite{Isola2011} or beauty~\cite{schifanella2015image} and a number of other intangible properties that are conveyed by images, especially in urban settings~\cite{khosla2014makes,quercia2014aesthetic,naik2014streetscore,parislooklikeparis}. Previous work has attempted to build on top of those efforts by producing inferential explanations: the input is selectively perturbed and those correlations are sought between the perturbations the varying output {\color{comment}[citation needed here]}. Although this type of approaches might unveil some of the influencing factors of the model, they still fail to explain the knowledge that the network has learned and which it uses to drive the decisions that it takes. 
%
%With this work we aim at moving an additional step towards explainable deep neural network models for computer vision. We propose a generalizable pipeline for capturing and explaining any intangible concept conveyed by a geo-referenced image as a whole (e.g., aesthetics of a urban scene). The pipeline learns the concept and then for a given street-view image finds an approximate transformation so as to maximize or minimize the presence of said affect. The is done by generating synthetic images to maximize the affect and then finding a best match to the synthetic images from the street-view database under some compositional constraints. We show that the humans overwhelmingly agree with the pipeline's transformations using a crowd sourcing experiment and then show that the transformations of the pipeline can be explained using well known urban design metrics.
%
%{\color{comment}[LUCA: the explainability of the pipeline seems an addition to the main contribution, which is the image transformation. So I wonder if all the motivation and discussion should be shifted towards that instead of focusing it on explainability.]}
%============================


The development of new technologies behind deep neural networks has progressed at an amazing pace over the past decade~\cite{lecun2015deep}, thus enabling the research community to achieve groundbreaking results across fields and in a wide variety of tasks including object detection~\cite{szegedy2013deep}, scene detection, reinforcement learning~\cite{mnih2015human}, and language modeling~\cite{arisoy2012deep}. Where deep learning excels in performance -- in many cases surpassing human ability -- it often falls short in producing models that are interpretable. As a result, unlike in traditional machine learning, neural networks are often used as black-box functions whose decisions cannot be supplemented by any human-readable explanation about \textit{why} those decisions were taken. This limitation is known as the \textit{explainability gap} or the \textit{black-box problem} of neural networks. The intrinsic complexity behind deep neural networks makes this problem hard to solve. However, as the adoption deep neural networks spreads across domains and as their use becomes more common to solve increasingly sophisticated duties with humans in the loop, it becomes crucial to annotate any decision with a human-readable explanation.
The rapid growth in feasibility and potential of deeplearning models have also spurred several cross disciplinary application oriented research. We particular bring to notice the works like \cite{naik2014streetscore,seresinhe2017using,Law:2017:ACN:3149808.3149810,seresinhe2015quantifying,naik2017computer} which have used deeplearning in the areas of quantification of urban perception and urban spacial reasoning . These works draw inferences using deeplearning driven classifiers about several aspects of urban environment such as scenic-ness , deprivation or changes in terms of land use.  However these also suffer from the problem of explaining why a particular inference is drawn. These insights are of significant importance as they can be fed back into the collection of intervention systems like urban planning departments, urban activists, municipal authorities etc. More so these explainable insights also need to be interpretable by the urban intervention agents.  
In this paper we propose a generalizable pipeline powered by deep learning that produces interpretable insights on what makes an urban environment arouse a particular perception. We do so by reasoning with the knowledge learnt by a generative deep learning model, trained to quantify a particular dimension of urban environment such as scenic-ness, beauty, safety etc. We reason using urban design metrics drawn from literature which are measured in actual streetview images using different computer vision techniques. For the sake of focus, in this paper we concentrate on the urban perception of beauty, but essentially this pipeline is generalizable to any aforementioned dimension, given a required dataset. 

In the following sections, we will explore the related work in the fields of deep learning driven aesthetic computing and urban analysis. We then elaborate on the data, design and technical details of the \textit{FaceLift} pipeline. We then describe the design and implementation of explainable urban design metrics. We then evaluate the behaviour of these metrics with respect to the data. Finally we discuss the expert views about such a tool and discuss the biases and implications.
