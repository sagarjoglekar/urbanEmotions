\section{Related Work}
\label{sec:related}
Previous work has focused on collecting ground truth data about how people perceive urban spaces, on predicting urban qualities (including beauty) from visual data, and on generating synthetic images that enhance a given quality (e.g., beauty). 


\mbox{}\\
\noindent
\textbf{Ground truth of urban perceptions.} So far the most detailed studies of perceptions of urban environments and their visual appearance have relied on personal interviews and observation of city streets: for example, some researchers relied on annotations of video recordings by experts~\cite{sampson04seeing}, while others have used participant ratings of simulated (rather than existing) street scenes~\cite{lindal2012}. The web has recently been used to survey a large number of individuals. Place Pulse is a website that asks a series of binary perception questions (such as `Which place looks safer [between the two]?') across a large number of geo-tagged images~\cite{salesses2013collaborative}. In a similar way, Quercia \emph{et al.} collected pairwise judgments about the extent to which urban scenes are considered quiet, beautiful and happy~\cite{quercia2014aesthetic}. They were then able to analyse the scenes together with their ratings using image-processing tools, and found that the amount of greenery in any given scene was associated with all three attributes and that cars and fortress-like buildings were associated with sadness. Taken all together, the results pointed in the same direction: urban elements that hinder social interactions were undesirable, while elements that increase interactions were the ones that should be integrated by urban planners to retrofit cities for greater happiness. 

\mbox{}\\
\noindent
\textbf{Deep learning and the city.} Computer vision techniques have increasingly become more sophisticated. Deep learning techniques have been used to accurately predict urban beauty~\cite{dubey2016deep,seresinhe2017using}, urban change~\cite{naik2017computer}, and even crime~\cite{DeNadai16}.

\mbox{}\\
\noindent
\textbf{Generative models.} Deep learning has recently been used not only to analyze existing images but also to generate new ones. Ngyuen \emph{et al.}~\cite{nguyen2016synthesizing} used generative networks to create natural-looking image that maximizes a specific neuron. In theory, the resulting image is the one that ``best activates'' the neuron under consideration (e.g., that associated with urban beauty). In practice, it is still a synthetic image that needs further processing to look realistic. \mbox{} \\

\mbox{}
To sum up, a lot of work has gone into collecting ground truth data about how people tend to perceive urban spaces, and into building accurate predictions models of urban qualities. However,  little work has gone into models that generate realistic urban scenes enhancing desirable qualities and that offer human-interpretable explanations of what they generate. 







%We explore related work in the fields of computational aesthetics and in the area of data driven inferences in urban environments.  Early work in the field of computational aesthetics done by Datta \cite{datta2008algorithmic} looked at the beauty aspect of images using hand-crafted visual features and datasets collected from photo-contest websites. It showed that subjective properties like beauty can be estimated using computer vision techniques, provided we have good data.  The introduction of deep-learning in this field boosted the activity. Post deep-learning works \cite{khosla2014makes,Wang:2015:USA:2832415.2832579,schifanella2015image} explored the dimensions of beauty, aesthetics and their linkages to popularity and engagement over the web. Despite being very subjective dimensions, these works showed impressive performance in quantifying them. 
%But all of them have a gap in explaining why their models have a good performance and what features have the classifiers learnt to look for. These questions boil down to the concept of explainability of machine learning models. 
%In the past few years, there has been some progress made in the field of computational aesthetics. The work done by Datta \cite{datta2008algorithmic} looked at the beauty aspect of images by using crowd sourced annotation and then building classifiers on top. The introduction of deep-learning to this field boosted the activity. Works such as \cite{Isola2011} used it to understand memorability. Some other works like \cite{khosla2014makes} \cite{Wang:2015:USA:2832415.2832579} \cite{schifanella2015image} , explored the dimensions of beauty, aesthetics and their linkages to popularity and engagement over the web. The work by Redi \cite{redi20146} looked at quantification of the notion of creativity in the short microvideos. These works look at properties which are abstract and very subjective. But still they all claim impressive performances in these aspects. But all of them have a gap in explaining why their models have a good performance and what features have the classifiers learnt to look for. These questions boil down to the concept of explainability of machine learning models. 
%\par
%Extending quantification of subjective information to the realm of maps was explored by works such as \cite{quercia2014shortest,quercia2015chatty,quercia2015smelly,aiello2016chatty}. These works took the subjective dimensions such as beauty, loudness, and smelly-ness and augment this information onto real world maps to present a new dimension in which one can explore their world.
%Works like \cite{naik2014streetscore,salesses2013collaborative}, collected and analyzed responses to images of urbanscapes across different subjective dimensions including safety, depression, beauty and built deep-learning models on their data. An extension of this work  \cite{dubey2016deep} used deep learning  to train models capable to rank urban images according to these subjective dimensions. 

%The questions pertaining to what the network learns semantically have been explored for popular use cases \cite{mao2014explain,karpathy2015deep}  but still remain largely unexplored for intangible classes representing concepts like beauty, sentiment etc.  In this paper, we apply this general scheme to the specific problem of predicting beauty and explaining the changes that influence the beautification process. 

%In the past couple of years, there have been papers which exploit generative version of neural nets to delve into the aspects of explainability.
%The design of GAN inherently encodes the knowledge learned by a neural network from the distribution of training data into a form of code based generator \cite{goodfellow2014generative}. To build on to of the generative models, the paper by Ngyuen et.al \cite{nguyen2016synthesizing} looks at using generative networks to create the best Natural-like image that maximizes a particular neuron in the network. The resulting image can be imagined as the image of the cumulative knowledge learned by the network that activates the neuron under consideration. If this neuron is the output label neuron, the resulting images summarize the knowledge of the network that describes a particular label.

%In the area of urban perception and urban affects, some recent works have shown some progress. Works like Street score \cite{naik2014streetscore} and  \cite{salesses2013collaborative}, have demonstrated innovative techniques of collecting urban perception data. They also did some interesting analysis of the data to understand how safety, depression, beauty and other such dimensions are perceived across urban spaces. An extension work  \cite{dubey2016deep} also utilized deep learning methods to train models capable of comparing two urban images for their perception values in terms of beauty et.al. However even these works did not dive into the reasoning aspect of these models.
%In the past couple of years, there have been papers which exploit generative version of neural nets to delve into the aspects of explainability.
%The design of GAN inherently encodes the knowledge learned by a neural network from the distribution of training data into a form of code based generator \cite{goodfellow2014generative}. To build on to of the generative models, the paper by Ngyuen et.al \cite{nguyen2016synthesizing} looks at using generative networks to create the best Natural-like image that maximizes a particular neuron in the network. The resulting image can be imagined as the image of the cumulative knowledge learned by the network that activates the neuron under consideration. If this neuron is the output label neuron, the resulting images summarize the knowledge of the network that describes a particular label.

%\par
%The literature discussed here shows that there are gaps in understanding of the models that do very well when it comes to perceptual properties. On the other hand, we also see that there has been some progress in visualizing and understanding the internal reasoning of neural networks \textbf{[REF]}. We exploit recent developments in this field to \textbf{[...]} 
%This motivates our work, which proposes a series of steps comprising a pipeline, which can streamline the task of explaining computational aesthetics models. A more specific use case of this is understanding urban properties.